{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import (\n",
    "    DataLoader, \n",
    "    Dataset\n",
    ")\n",
    "from matplotlib import pyplot as plt\n",
    "from albumentations import (\n",
    "    HorizontalFlip, \n",
    "    ShiftScaleRotate, \n",
    "    Normalize, \n",
    "    Resize, \n",
    "    Compose, \n",
    "    GaussNoise,\n",
    "    RandomContrast,\n",
    "    RandomGamma,\n",
    "    RandomBrightness,\n",
    "    OneOf,\n",
    "    ElasticTransform,\n",
    "    GridDistortion,\n",
    "    OpticalDistortion\n",
    ")\n",
    "from albumentations.torch import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "gpu = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "IMAGE_SIZE = 768\n",
    "BATCH_SIZE = 8\n",
    "GRAD_ACCUM = 4\n",
    "MODEL_NAME = \"se_resnext50_32x4d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_decode(rle, height=1024, width=1024, fill_value=1):\n",
    "    component = np.zeros((height, width), np.float32)\n",
    "    component = component.reshape(-1)\n",
    "    \n",
    "    rle = np.array([int(s) for s in rle.strip().split(' ')])\n",
    "    rle = rle.reshape(-1, 2)\n",
    "    \n",
    "    start = 0\n",
    "    for index, length in rle:\n",
    "        start = start + index\n",
    "        end = start + length\n",
    "        component[start: end] = fill_value\n",
    "        start = end\n",
    "    \n",
    "    component = component.reshape(width, height).T\n",
    "    return component\n",
    "\n",
    "\n",
    "def run_length_encode(component):\n",
    "    component = component.T.flatten()\n",
    "    \n",
    "    start = np.where(component[1:] > component[:-1])[0] + 1\n",
    "    end = np.where(component[:-1] > component[1:])[0] + 1\n",
    "    length = end - start\n",
    "    rle = []\n",
    "    for i in range(len(length)):\n",
    "        if i == 0:\n",
    "            rle.extend([start[0], length[0]])\n",
    "        else:\n",
    "            rle.extend([start[i] - end[i-1], length[i]])\n",
    "    rle = ' '.join([str(r) for r in rle])\n",
    "    return rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIIMDataset(Dataset):\n",
    "    def __init__(self, df, data_folder, size, mean, std, phase):\n",
    "        self.df = df\n",
    "        self.data_folder = data_folder\n",
    "        self.size = size\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.phase = phase\n",
    "        self.transforms = get_transforms(phase, size, mean, std)\n",
    "        self.gb = self.df.groupby('ImageId')\n",
    "        self.fnames = list(self.gb.groups.keys())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.fnames[idx]\n",
    "        df = self.gb.get_group(image_id)\n",
    "        annotations = df[' EncodedPixels'].tolist()\n",
    "        image_path = os.path.join(self.data_folder, image_id + \".png\")\n",
    "        image = cv2.imread(image_path)\n",
    "        mask = np.zeros([1024, 1024])\n",
    "        \n",
    "        if annotations[0] != ' -1':\n",
    "            for rle in annotations:\n",
    "                mask += run_length_decode(rle)\n",
    "                \n",
    "        mask = (mask >= 1).astype('float32') \n",
    "        augmented = self.transforms(image=image, mask=mask)\n",
    "        \n",
    "        image = augmented['image']\n",
    "        mask = augmented['mask']\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "\n",
    "\n",
    "def get_transforms(phase, size, mean, std):\n",
    "    list_transforms = []\n",
    "    if phase == \"train\":\n",
    "        list_transforms.extend([\n",
    "            ShiftScaleRotate(\n",
    "                shift_limit=0,  # no resizing\n",
    "                scale_limit=0.1,\n",
    "                rotate_limit=10, # rotate\n",
    "                p=0.5,\n",
    "                border_mode=cv2.BORDER_CONSTANT\n",
    "            ),\n",
    "            OneOf([\n",
    "                RandomContrast(),\n",
    "                RandomGamma(),\n",
    "                RandomBrightness(),\n",
    "                ], p=np.random.choice([.1, .2, .3])\n",
    "            ),\n",
    "            OneOf([\n",
    "                ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "                GridDistortion(),\n",
    "                OpticalDistortion(distort_limit=2, shift_limit=0.5),\n",
    "                ], p=np.random.choice([.1, .2, .3])\n",
    "            )\n",
    "        ])\n",
    "    list_transforms.extend([\n",
    "        Resize(size, size),\n",
    "        Normalize(mean=mean, std=std, p=1),\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "    list_trfms = Compose(list_transforms)\n",
    "    return list_trfms\n",
    "\n",
    "\n",
    "\n",
    "def provider(bag, fold, total_folds, data_folder, df_path, phase, size, mean=None, std=None, \n",
    "             batch_size=8, num_workers=4):\n",
    "    df = pd.read_csv(df_path)\n",
    "    \n",
    "    df_with_mask = df[df[\" EncodedPixels\"] != \" -1\"]\n",
    "    df_with_mask['has_mask'] = 1\n",
    "    \n",
    "    df_without_mask = df[df[\" EncodedPixels\"] == \" -1\"]\n",
    "    df_without_mask['has_mask'] = 0\n",
    "    df_without_mask_sampled = df_without_mask.sample(len(df_with_mask.drop_duplicates('ImageId')))\n",
    "    \n",
    "    df = pd.concat([df_with_mask, df_without_mask_sampled])\n",
    "    \n",
    "    kfold = StratifiedKFold(total_folds, shuffle=True, random_state=bag)\n",
    "    train_idx, val_idx = list(kfold.split(df[\"ImageId\"], df[\"has_mask\"]))[fold]\n",
    "    train_df, val_df = df.iloc[train_idx], df.iloc[val_idx]\n",
    "\n",
    "    if phase == \"train\":\n",
    "        df = train_df  \n",
    "    else:\n",
    "        df = val_df\n",
    "    \n",
    "    image_dataset = SIIMDataset(df, data_folder, size, mean, std, phase)\n",
    "    dataloader = DataLoader(image_dataset, batch_size=batch_size, num_workers=num_workers, \n",
    "                            pin_memory=True, shuffle=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "            \n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.focal = FocalLoss(gamma)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, threshold):\n",
    "    X_p = np.copy(X)\n",
    "    preds = (X_p > threshold).astype('uint8')\n",
    "    return preds\n",
    "\n",
    "\n",
    "def metric(probability, truth, threshold=0.5, reduction='none'):\n",
    "    '''Calculates dice of positive and negative images seperately'''\n",
    "    '''probability and truth must be torch tensors'''\n",
    "    batch_size = len(truth)\n",
    "    with torch.no_grad():\n",
    "        probability = probability.view(batch_size, -1)\n",
    "        truth = truth.view(batch_size, -1)\n",
    "        assert(probability.shape == truth.shape)\n",
    "\n",
    "        p = (probability > threshold).float()\n",
    "        t = (truth > 0.5).float()\n",
    "\n",
    "        t_sum = t.sum(-1)\n",
    "        p_sum = p.sum(-1)\n",
    "        neg_index = torch.nonzero(t_sum == 0)\n",
    "        pos_index = torch.nonzero(t_sum >= 1)\n",
    "\n",
    "        dice_neg = (p_sum == 0).float()\n",
    "        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n",
    "\n",
    "        dice_neg = dice_neg[neg_index]\n",
    "        dice_pos = dice_pos[pos_index]\n",
    "        dice = torch.cat([dice_pos, dice_neg])\n",
    "\n",
    "        dice_neg = np.nan_to_num(dice_neg.mean().item(), 0)\n",
    "        dice_pos = np.nan_to_num(dice_pos.mean().item(), 0)\n",
    "        dice = dice.mean().item()\n",
    "\n",
    "        num_neg = len(neg_index)\n",
    "        num_pos = len(pos_index)\n",
    "\n",
    "    return dice, dice_neg, dice_pos, num_neg, num_pos\n",
    "\n",
    "\n",
    "class Meter:\n",
    "    '''A meter to keep track of iou and dice scores throughout an epoch'''\n",
    "    def __init__(self, phase, epoch):\n",
    "        self.base_threshold = 0.5\n",
    "        self.base_dice_scores = []\n",
    "        self.dice_neg_scores = []\n",
    "        self.dice_pos_scores = []\n",
    "        self.iou_scores = []\n",
    "\n",
    "    def update(self, targets, outputs):\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        dice, dice_neg, dice_pos, _, _ = metric(probs, targets, self.base_threshold)\n",
    "        self.base_dice_scores.append(dice)\n",
    "        self.dice_pos_scores.append(dice_pos)\n",
    "        self.dice_neg_scores.append(dice_neg)\n",
    "        preds = predict(probs, self.base_threshold)\n",
    "        iou = compute_iou_batch(preds, targets, classes=[1])\n",
    "        self.iou_scores.append(iou)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        dice = np.mean(self.base_dice_scores)\n",
    "        dice_neg = np.mean(self.dice_neg_scores)\n",
    "        dice_pos = np.mean(self.dice_pos_scores)\n",
    "        dices = [dice, dice_neg, dice_pos]\n",
    "        iou = np.nanmean(self.iou_scores)\n",
    "        return dices, iou\n",
    "\n",
    "    \n",
    "def epoch_log(phase, epoch, epoch_loss, meter, start):\n",
    "    '''logging the metrics at the end of an epoch'''\n",
    "    dices, iou = meter.get_metrics()\n",
    "    dice, dice_neg, dice_pos = dices\n",
    "    print(\"Loss: %0.4f | dice: %0.4f | dice_neg: %0.4f | dice_pos: %0.4f | IoU: %0.4f\" % \n",
    "          (epoch_loss, dice, dice_neg, dice_pos, iou))\n",
    "    return dice, iou\n",
    "\n",
    "\n",
    "def compute_ious(pred, label, classes, ignore_index=255, only_present=True):\n",
    "    '''computes iou for one ground truth mask and predicted mask'''\n",
    "    pred[label == ignore_index] = 0\n",
    "    ious = []\n",
    "    for c in classes:\n",
    "        label_c = label == c\n",
    "        if only_present and np.sum(label_c) == 0:\n",
    "            ious.append(np.nan)\n",
    "            continue\n",
    "        pred_c = pred == c\n",
    "        intersection = np.logical_and(pred_c, label_c).sum()\n",
    "        union = np.logical_or(pred_c, label_c).sum()\n",
    "        if union != 0:\n",
    "            ious.append(intersection / union)\n",
    "    return ious if ious else [1]\n",
    "\n",
    "\n",
    "def compute_iou_batch(outputs, labels, classes=None):\n",
    "    '''computes mean iou for a batch of ground truth masks and predicted masks'''\n",
    "    ious = []\n",
    "    preds = np.copy(outputs) # copy is imp\n",
    "    labels = np.array(labels) # tensor to np\n",
    "    for pred, label in zip(preds, labels):\n",
    "        ious.append(np.nanmean(compute_ious(pred, label, classes)))\n",
    "    iou = np.nanmean(ious)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    '''This class takes care of training and validation of our model'''\n",
    "    def __init__(self, model, bag=0, fold=0, batch_size=BATCH_SIZE, grad_accum=GRAD_ACCUM, image_size=IMAGE_SIZE, \n",
    "                 max_epochs=50, lr=5e-4, early_stopping=7):\n",
    "        self.bag = bag\n",
    "        self.fold = fold\n",
    "        self.total_folds = 5\n",
    "        self.phases = [\"train\", \"val\"]\n",
    "        \n",
    "        self.device = 'cuda'\n",
    "        self.num_workers = 8\n",
    "        torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "        cudnn.benchmark = True\n",
    "        \n",
    "        self.batch_size = {\"train\": batch_size, \"val\": batch_size}\n",
    "        self.accumulation_steps = grad_accum\n",
    "        self.num_epochs = max_epochs\n",
    "        \n",
    "        self.net = model.to(self.device)\n",
    "        self.lr = lr\n",
    "        self.criterion = MixedLoss(10.0, 2.0)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"max\", patience=3, verbose=True)\n",
    "        self.best_dice = float(\"-inf\")\n",
    "\n",
    "        self.dataloaders = {\n",
    "            phase: provider(\n",
    "                bag=bag,\n",
    "                fold=fold,\n",
    "                total_folds=5,\n",
    "                data_folder=data_folder,\n",
    "                df_path=train_rle_path,\n",
    "                phase=phase,\n",
    "                size=image_size,\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225),\n",
    "                batch_size=self.batch_size[phase],\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "            for phase in self.phases\n",
    "        }\n",
    "        self.losses = {phase: [] for phase in self.phases}\n",
    "        self.iou_scores = {phase: [] for phase in self.phases}\n",
    "        self.dice_scores = {phase: [] for phase in self.phases}\n",
    "        self.early_stopping = early_stopping\n",
    "        \n",
    "    def forward(self, images, targets):\n",
    "        images = images.to(self.device)\n",
    "        masks = targets.to(self.device)\n",
    "        outputs = self.net(images)\n",
    "        loss = self.criterion(outputs, masks)\n",
    "        return loss, outputs\n",
    "\n",
    "    def iterate(self, epoch, phase):\n",
    "        meter = Meter(phase, epoch)\n",
    "        \n",
    "        self.net.train(phase == \"train\")\n",
    "        \n",
    "        batch_size = self.batch_size[phase]\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        total_batches = len(dataloader)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        print(f\"Starting epoch: {epoch} | phase: {phase} | â°: {start}\")\n",
    "        for itr, batch in tqdm(enumerate(dataloader), desc=phase, total=total_batches, leave=False):\n",
    "            images, targets = batch\n",
    "\n",
    "            if phase == \"train\":\n",
    "                loss, outputs = self.forward(images, targets)\n",
    "                loss.backward()\n",
    "                if (itr + 1 ) % self.accumulation_steps == 0:\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    loss, outputs = self.forward(images, targets)\n",
    "            \n",
    "            loss = loss / self.accumulation_steps\n",
    "            running_loss += loss.item()\n",
    "            outputs = outputs.detach().cpu()\n",
    "            meter.update(targets, outputs)\n",
    "\n",
    "        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n",
    "        dice, iou = epoch_log(phase, epoch, epoch_loss, meter, start)\n",
    "        self.losses[phase].append(epoch_loss)\n",
    "        self.dice_scores[phase].append(dice)\n",
    "        self.iou_scores[phase].append(iou)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        return epoch_loss, dice\n",
    "\n",
    "    def start(self):\n",
    "        best_epoch = 0\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.iterate(epoch, \"train\")\n",
    "            state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"best_dice\": self.best_dice,\n",
    "                \"state_dict\": self.net.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict(),\n",
    "            }\n",
    "            val_loss, val_dice = self.iterate(epoch, \"val\")\n",
    "            self.scheduler.step(val_dice)\n",
    "\n",
    "            if val_dice > self.best_dice:\n",
    "                print(\"******** New optimal found, saving state ********\")\n",
    "                state[\"best_dice\"] = self.best_dice = val_dice\n",
    "                torch.save(state, \"./model_{}_size_{}_bag_{}_fold_{}.pth\".format(self.net.name, IMAGE_SIZE, self.bag, self.fold))\n",
    "                best_epoch = epoch\n",
    "            else:\n",
    "                if epoch - best_epoch > self.early_stopping:\n",
    "                    print('********early stopping triggered')\n",
    "                    break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_folder, df, size, mean, std, tta=4):\n",
    "        self.data_folder = data_folder\n",
    "        self.size = size\n",
    "        self.fnames = list(df[\"ImageId\"])\n",
    "        self.num_samples = len(self.fnames)\n",
    "        self.transform = Compose(\n",
    "            [\n",
    "                Resize(size, size),\n",
    "                Normalize(mean=mean, std=std, p=1),\n",
    "                ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.fnames[idx]\n",
    "        path = os.path.join(self.data_folder, fname + \".png\")\n",
    "        image = cv2.imread(path)\n",
    "        images = self.transform(image=image)[\"image\"]\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "\n",
    "def post_process(probability, threshold, min_size):\n",
    "    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "    predictions = np.zeros((1024, 1024), np.float32)\n",
    "    num = 0\n",
    "    for c in range(1, num_component):\n",
    "        p = (component == c)\n",
    "        if p.sum() > min_size:\n",
    "            predictions[p] = 1\n",
    "            num += 1\n",
    "    return predictions, num\n",
    "\n",
    "\n",
    "def plot(scores, name):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(range(len(scores[\"train\"])), scores[\"train\"], label=f'train {name}')\n",
    "    plt.plot(range(len(scores[\"train\"])), scores[\"val\"], label=f'val {name}')\n",
    "    plt.title(f'{name} plot'); plt.xlabel('Epoch'); plt.ylabel(f'{name}');\n",
    "    plt.legend(); \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_path = '../data/sample_submission.csv'\n",
    "train_rle_path = '../data/train-rle.csv'\n",
    "data_folder = \"../data/train_png\"\n",
    "test_data_folder = \"../data/test_png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "\n",
    "for bag in range(5):\n",
    "    for fold in range(5):\n",
    "        print(\"---- Start training on fold {}\".format(fold))\n",
    "\n",
    "        model = smp.Unet(MODEL_NAME, encoder_weights=\"imagenet\", activation=None)\n",
    "        model_trainer = Trainer(model, fold=fold)\n",
    "        model_trainer.start()\n",
    "\n",
    "        losses = model_trainer.losses\n",
    "        dice_scores = model_trainer.dice_scores \n",
    "        iou_scores = model_trainer.iou_scores\n",
    "\n",
    "        plot(losses, \"BCE loss\")\n",
    "        plot(dice_scores, \"Dice score\")\n",
    "        plot(iou_scores, \"IoU score\")\n",
    "        plt.show()\n",
    "\n",
    "        size = IMAGE_SIZE\n",
    "        batch_size = BATCH_SIZE\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        num_workers = 8\n",
    "        best_threshold = 0.5\n",
    "        min_size = 3500\n",
    "        device = 'cuda'\n",
    "\n",
    "        df = pd.read_csv(sample_submission_path)\n",
    "        testset = DataLoader(\n",
    "            TestDataset(test_data_folder, df, size, mean, std),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        model = model_trainer.net\n",
    "        state = torch.load('./model_{}_size_{}_bag_{}_fold_{}.pth'.format(model.name, IMAGE_SIZE, bag, fold), \n",
    "                           map_location=lambda storage, loc: storage)\n",
    "        model.load_state_dict(state[\"state_dict\"])\n",
    "        model.eval()\n",
    "        fold_preds = []\n",
    "        encoded_pixels = []\n",
    "        for i, batch in enumerate(tqdm(testset)):\n",
    "            with torch.no_grad():\n",
    "                preds = torch.sigmoid(model(batch.to(device)))\n",
    "            preds = preds.detach().cpu().numpy()[:, 0, :, :] # (batch_size, 1, size, size) -> (batch_size, size, size)\n",
    "            fold_preds.append(preds)\n",
    "\n",
    "            for probability in preds:\n",
    "                if probability.shape != (1024, 1024):\n",
    "                    probability = cv2.resize(probability, dsize=(1024, 1024), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "                prediction, num_predict = post_process(probability, best_threshold, min_size)\n",
    "\n",
    "                if num_predict == 0:\n",
    "                    encoded_pixels.append('-1')\n",
    "                else:\n",
    "                    r = run_length_encode(prediction)\n",
    "                    encoded_pixels.append(r)\n",
    "\n",
    "        fold_preds = np.vstack(fold_preds)\n",
    "        df['EncodedPixels'] = encoded_pixels\n",
    "        df.to_csv('model_{}_size_{}_bag_{}_fold_{}.csv'.format(model.name, IMAGE_SIZE, bag, fold), \n",
    "                  columns=['ImageId', 'EncodedPixels'], index=False)    \n",
    "\n",
    "        test_preds.append(fold_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_preds = test_preds[0] / 5\n",
    "for test_pred in test_preds[1:5]:\n",
    "    avg_preds += test_pred / 5\n",
    "\n",
    "encoded_pixels = []\n",
    "for probability in avg_preds:\n",
    "    if probability.shape != (1024, 1024):\n",
    "        probability = cv2.resize(probability, dsize=(1024, 1024), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    prediction, num_predict = post_process(probability, best_threshold, min_size)\n",
    "\n",
    "    if num_predict == 0:\n",
    "        encoded_pixels.append('-1')\n",
    "    else:\n",
    "        r = run_length_encode(prediction)\n",
    "        encoded_pixels.append(r)\n",
    "\n",
    "df['EncodedPixels'] = encoded_pixels\n",
    "df.to_csv('model_{}_size_{}_avg.csv'.format(model.name, IMAGE_SIZE), \n",
    "          columns=['ImageId', 'EncodedPixels'], index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_preds = test_preds[0] / len(test_preds)\n",
    "for test_pred in test_preds[1:]:\n",
    "    avg_preds += test_pred / len(test_preds)\n",
    "\n",
    "encoded_pixels = []\n",
    "for probability in avg_preds:\n",
    "    if probability.shape != (1024, 1024):\n",
    "        probability = cv2.resize(probability, dsize=(1024, 1024), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    prediction, num_predict = post_process(probability, best_threshold, min_size)\n",
    "\n",
    "    if num_predict == 0:\n",
    "        encoded_pixels.append('-1')\n",
    "    else:\n",
    "        r = run_length_encode(prediction)\n",
    "        encoded_pixels.append(r)\n",
    "\n",
    "df['EncodedPixels'] = encoded_pixels\n",
    "df.to_csv('model_{}_size_{}_bagged_avg.csv'.format(model.name, IMAGE_SIZE), \n",
    "          columns=['ImageId', 'EncodedPixels'], index=False)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
